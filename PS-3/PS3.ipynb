{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0JU1b-ZOdhb",
        "outputId": "b7cf0635-83b0-41d7-b8ee-75545f791ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jftI_Oh9PE-Q",
        "outputId": "8c9d8014-f85c-4c21-f433-aedc448ccd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PS3_train.csv\")\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "H6jkptibPJ9g",
        "outputId": "ebaa95c9-57df-445b-c002-963611db5a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 35,112\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 content  \\\n",
              "26470  Standard flat washers are an economical choice...   \n",
              "7920   Our finest shirt fabric: Imperial 100s cotton ...   \n",
              "22618       PS-12180NB 12v 18Ah Lead Acid Battery 12VOLT   \n",
              "7599   Danea Gorbett draws on her background in psych...   \n",
              "34198  Bio-balls do not require replacement unless da...   \n",
              "25655  I really recommend it... Have fun learning usi...   \n",
              "28289  \"...accessible...comprehensive but largely jar...   \n",
              "20631  A century of Transylvanian tranquility is abou...   \n",
              "11376  Distractingly loose but clever, this 1987 come...   \n",
              "29879  This polyoxymethylene spur gear with 20-degree...   \n",
              "\n",
              "                                                   title         uid  \\\n",
              "26470  Nylon 6/6 Flat Washer, #6, 0.15&#034; ID, 0.31...  B000FN15PQ   \n",
              "7920   Amazon.com: Imperial 100s European Straight Co...  B00008JP98   \n",
              "22618   Powersonic PS-12180NB 12v 18Ah Lead Acid Battery  B0002ILJZU   \n",
              "7599   Adopted Teens Only: A Survival Guide to Adoles...  0595325831   \n",
              "34198  Marineland PA11486 Canister Filter Bio-Balls P...  B000NRXB5G   \n",
              "25655                        HEBREW in 10 minutes a day®  0944502253   \n",
              "28289  The Observing Guide to the Messier Marathon: A...  0521803861   \n",
              "20631                Super Nintendo Super Castlevania IV  B000035XZD   \n",
              "11376                            Innerspace [VHS] (1987)  B000055YXK   \n",
              "29879  Spur Gear, 20 Degree Pressure Angle, Polyoxyme...  B000FMUNDC   \n",
              "\n",
              "       target_ind  \n",
              "26470         363  \n",
              "7920          131  \n",
              "22618         401  \n",
              "7599            6  \n",
              "34198         378  \n",
              "25655           5  \n",
              "28289          16  \n",
              "20631          74  \n",
              "11376         103  \n",
              "29879         357  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9105f663-248d-4a81-9e5d-db00def06c78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>title</th>\n",
              "      <th>uid</th>\n",
              "      <th>target_ind</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26470</th>\n",
              "      <td>Standard flat washers are an economical choice...</td>\n",
              "      <td>Nylon 6/6 Flat Washer, #6, 0.15&amp;#034; ID, 0.31...</td>\n",
              "      <td>B000FN15PQ</td>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7920</th>\n",
              "      <td>Our finest shirt fabric: Imperial 100s cotton ...</td>\n",
              "      <td>Amazon.com: Imperial 100s European Straight Co...</td>\n",
              "      <td>B00008JP98</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22618</th>\n",
              "      <td>PS-12180NB 12v 18Ah Lead Acid Battery 12VOLT</td>\n",
              "      <td>Powersonic PS-12180NB 12v 18Ah Lead Acid Battery</td>\n",
              "      <td>B0002ILJZU</td>\n",
              "      <td>401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7599</th>\n",
              "      <td>Danea Gorbett draws on her background in psych...</td>\n",
              "      <td>Adopted Teens Only: A Survival Guide to Adoles...</td>\n",
              "      <td>0595325831</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34198</th>\n",
              "      <td>Bio-balls do not require replacement unless da...</td>\n",
              "      <td>Marineland PA11486 Canister Filter Bio-Balls P...</td>\n",
              "      <td>B000NRXB5G</td>\n",
              "      <td>378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25655</th>\n",
              "      <td>I really recommend it... Have fun learning usi...</td>\n",
              "      <td>HEBREW in 10 minutes a day®</td>\n",
              "      <td>0944502253</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28289</th>\n",
              "      <td>\"...accessible...comprehensive but largely jar...</td>\n",
              "      <td>The Observing Guide to the Messier Marathon: A...</td>\n",
              "      <td>0521803861</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20631</th>\n",
              "      <td>A century of Transylvanian tranquility is abou...</td>\n",
              "      <td>Super Nintendo Super Castlevania IV</td>\n",
              "      <td>B000035XZD</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11376</th>\n",
              "      <td>Distractingly loose but clever, this 1987 come...</td>\n",
              "      <td>Innerspace [VHS] (1987)</td>\n",
              "      <td>B000055YXK</td>\n",
              "      <td>103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29879</th>\n",
              "      <td>This polyoxymethylene spur gear with 20-degree...</td>\n",
              "      <td>Spur Gear, 20 Degree Pressure Angle, Polyoxyme...</td>\n",
              "      <td>B000FMUNDC</td>\n",
              "      <td>357</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9105f663-248d-4a81-9e5d-db00def06c78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9105f663-248d-4a81-9e5d-db00def06c78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9105f663-248d-4a81-9e5d-db00def06c78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['info'] =df['title'] + df['content']\n",
        "sentences = df['info'].values\n",
        "labels = df['target_ind'].values"
      ],
      "metadata": {
        "id": "Y9htTtWUPm8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)"
      ],
      "metadata": {
        "id": "42F2X_S4PzS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "\n",
        "for sent in sentences:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,           \n",
        "                        padding = 'max_length',\n",
        "                        truncation = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    \n",
        "      \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp2nnj3EQJeI",
        "outputId": "fd6e0f0b-e311-4628-82dc-9c667bbcebc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Amazon.com: Wrangler Men's Rugged Wear Relaxed Fit Jean: ClothingPremium quality five pocket jean from Wrangler Rugged Wear. This Relaxed Fit Jean is made from 100% cotton denim for durability with extra room in the seat and thigh for comfort.\tMen's Wrangler Trail Trekker Relaxed Fit Jeans Set out on a long hike, or kick back for an afternoon full of watching college football from the comfort of your own home. These Wrangler Trail Trekker Relaxed Fit Jeans are up for anything you are! Check 'em out: 100% cotton denim construction; Relaxed 5 pocket style; Easy entry, extra deep front pockets; Solid brass YKK zip fly; Leather waistband patch; Fit easily over boots; Machine wash / dry. Imported. State Color and Size! Get yours today! Men's Wrangler 36\" Inseam Trail Trekker Relaxed Fit Jeans\n",
            "Token IDs: tensor([    0, 25146,     4,   175,    35, 23511, 32268,  4011,    18, 29599,\n",
            "         4462, 16915, 38924,   196, 14950,  5363,    35, 36221, 46025,  1318,\n",
            "          292,  7524,  1236, 12001,    31, 23511, 32268, 29599,  4462, 16915,\n",
            "            4,   152, 38924,   196, 14950,  5363,    16,   156,    31,   727,\n",
            "          207, 13178, 21288,    13, 28327,    19,  1823,   929,    11,     5,\n",
            "         2418,     8, 18781,    13,  5863,     4, 50117, 17762,    18, 23511,\n",
            "        32268,  8393, 20351,  5029, 38924,   196, 14950,  9925,  1253,  8504,\n",
            "           66,    15,    10,   251,  5960,     6,    50,  3151,   124,    13,\n",
            "           41,  1390,   455,     9,  2494,  1564,  1037,    31,     5,  5863,\n",
            "            9,   110,   308,   184,     4,  1216, 23511, 32268,  8393, 20351,\n",
            "         5029, 38924,   196, 14950,  9925,  1253,    32,    62,    13,   932,\n",
            "           47,    32,   328,  4254,   128,   991,    66,    35,   727,   207,\n",
            "        13178, 21288,  1663,   131, 38924,   196,   195,  7524,  2496,   131,\n",
            "        18609,  3555,     6,  1823,  1844,   760, 12189,   131, 19324, 20535,\n",
            "          854, 26228, 23595,  3598,   131, 29904, 13977,  9484,  9202,   131,\n",
            "        14950,  2773,    81, 10317,   131, 14969, 10397,  1589,  3841,     4,\n",
            "         5902, 31340,     4,   331, 16858,     8, 17736,   328,  2315, 14314,\n",
            "          452,   328,  4011,    18, 23511, 32268,  2491,   113,    96,  1090,\n",
            "          424,  8393, 20351,  5029, 38924,   196, 14950,  9925,  1253,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piBf5ln-RCw4",
        "outputId": "a328bd2a-eea1-4a18-a547-2b1a9384fcff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31,600 training samples\n",
            "3,512 validation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "metadata": {
        "id": "Ji1RBelfRPRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import RobertaForSequenceClassification, AdamW, RobertaConfig\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    \"roberta-base\", \n",
        "    num_labels = 500, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "8ZNr3X9DRTRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "zfyT_ETGUTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "     "
      ],
      "metadata": {
        "id": "lW_Mm4RYUcPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "\n",
        "    \n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "metadata": {
        "id": "aIA3crTOUg72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/weights_roberta_epochs11.pth'))\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    \n",
        "    t0 = time.time()\n",
        "\n",
        "    \n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = []\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        \n",
        "        if step % 40 == 0 and not step == 0:\n",
        "        \n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            print(f'acc = {(np.mean(total_train_accuracy))}')\n",
        "\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_train_accuracy.append(flat_accuracy(logits, label_ids))\n",
        "    \n",
        "    \n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "     \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    \n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  \n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/weights_roberta_epochs{epoch_i+10}.pth')\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
      ],
      "metadata": {
        "id": "Tn95WpyjUnRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PS3_test.csv\")\n",
        "\n",
        "\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df['info'] = df['title'] + df['content']\n",
        "\n",
        "sentences = df['info'].values\n",
        "labels = [0]*len(df)\n",
        "\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 256,           \n",
        "                        padding = 'max_length',\n",
        "                        truncation = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "batch_size = 16 \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "prediction_dataloader = DataLoader(prediction_data,  batch_size=batch_size, shuffle = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njMl0Hx2WHJ3",
        "outputId": "3aef0219-3bab-4a7f-d8fa-552572c11399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 8,106\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import torch.nn.functional as F\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/weights_roberta_epochs6.pth'))\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        " \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  \n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  \n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "  \n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  \n",
        "  predictions.append(F.softmax(logits, dim = 1))\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  \n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g309I-l9lD9U",
        "outputId": "fd5656a4-fe4a-4309-c666-2051c513ca9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 8,106 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs_robert = torch.cat(predictions)\n",
        "probs_robert = np.array(probs_robert.cpu())\n",
        "preds_robert = np.argmax(probs_robert, axis = 1) "
      ],
      "metadata": {
        "id": "J0LnhHMSmqAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_weights = pd.DataFrame({'uid': [t for t in df['uid']], 'target_ind': preds_robert})\n",
        "submission_weights.to_csv('/content/drive/MyDrive/submission_robert_10.csv')"
      ],
      "metadata": {
        "id": "5hPNn3bBlGPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}